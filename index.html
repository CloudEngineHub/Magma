
<!DOCTYPE html>
<html>
<head>
    <title>Magma: A Foundation Model for Multimodal AI Agents</title>

  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet"> -->

  <!-- <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="icon" href="./static/images/logo.png">



  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L7VEDHS6G8');
</script>


<body>
    <section class="hero">
      <div class="hero-body no-bottom-padding">
        <div class="container">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Magma: A Foundation Model for Multimodal AI Agents</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a target="_blank" href="https://jwyang.github.io/">Jianwei Yang</a><sup>*</sup><sup>1</sup><sup>†</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://cs-people.bu.edu/rxtan/">Reuben Tan</a><sup>1</sup><sup>†</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://qianhuiwu.github.io/">Qianhui Wu</a><sup>1</sup><sup>†</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://ruijiezheng.com/">Ruijie Zheng</a><sup>2</sup><sup>‡</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="">Baolin Peng</a><sup>1</sup><sup>‡</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://cheryyunl.github.io">Yongyuan Liang</a><sup>2</sup><sup>‡</sup>&nbsp;&nbsp;&nbsp;<br>

                  <a target="_blank" href="https://users.umiacs.umd.edu/~hal/">Yu Gu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://pages.cs.wisc.edu/~mucai/">Mu Cai</a><sup>3</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://seonghyeonye.github.io/">Seonghyeon Ye</a><sup>4</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://joeljang.github.io/">Joel Jang</a><sup>5</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="">Yuquan Deng</a><sup>5</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://sites.google.com/site/larsliden">Lar Liden</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
                  <a target="_blank" href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao</a><sup>1</sup><sup>▽</sup>&nbsp;&nbsp;&nbsp;<br>
                  

                  <br><sup>1</sup>Microsoft Research&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>University of Maryland&nbsp;&nbsp;&nbsp;&nbsp;<sup>3</sup>University of  Wisconsin-Madison&nbsp;&nbsp;&nbsp;&nbsp;<sup>4</sup>KAIST&nbsp;&nbsp;&nbsp;&nbsp;<sup>5</sup>University of Washington
                  <br>&ast; Project lead.&nbsp;&nbsp;&nbsp;&nbsp;<sup>†</sup>First authors.&nbsp;&nbsp;&nbsp;&nbsp;<sup>‡</sup>Second authors.&nbsp;&nbsp;&nbsp;&nbsp;<sup>▽</sup>Leadership
                </span>
              </div>
              <div class="column has-text-centered">


                <div class="publication-links">
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/2406.00439"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->
  
                  <!-- arXiv Link. -->
                  <span class="link-block">
                    <a target="_blank" href=""
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="ai ai-arxiv"></i>
                      </span>
                      <span>ArXiv</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/microsoft/Magma"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                      </a>
                    </span>
                    <!-- Dataset Link. -->
                    <span class="link-block">
                      <a href=""
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <img src="static/images/hf_icon.svg" />
                        </span>
                        <span>Models</span>
                        </a>
                      </span>

                  <span class="link-block">
                    <a target="_blank" href=""
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-twitter"></i>
                      </span>
                      <span>Twitter</span>
                    </a>
                  </span>
                </div>
    
              </div>
              </div>
    
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
  



  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div align="center">
          <img src="./static/images/magma_teaser.png" alt="Image description" width="100%">
        </div>  
        <br> 
        <!-- <h2 class="subtitle has-text-centered"> -->
        <h2 class="subtitle">
          <strong>Magma</strong> is the <span style="color: rgb(182, 30, 30);"><em>first</em></span> foundation model that is capable of interpreting and grounding multimodal inputs within its environment. Given a described goal, <strong>Magma</strong> is able to formulate plans and execute actions to achieve it. By effectively transferring knowledge from freely available visual and language data, <strong>Magma</strong> bridges verbal, spatial and temporal intelligence to navigate complex tasks and settings.
        </h2>
        
      </div>
  
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-two-thirds">
        <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
            <h2 class="title is-3">Abstract</h2>
          </div>
      </div>
    
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds ">
          <div class="content has-text-justified">
            We present <strong>Magma</strong>, a foundation model serving multimodal AI agentic tasks in both the digital and physical worlds.  <strong>Magma</strong> is a significant extension of vision-language (VL) models in that the former not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial intelligence) and to complete agentic tasks ranging from UI navigation to robot manipulation.
            <strong>Magma</strong> is pretrained on large amounts of heterogeneous VL datasets including images, videos and robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) and the object movements (e.g., the trace of a robotic arm) in videos are labeled by Trace-of-Mark (ToM). 
            Extensive experiments show that SoM and ToM facilitate the acquisition of spatial intelligence from large-scale training data.
            <strong>Magma</strong> creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are tailored specifically to these tasks. On VL tasks, Magma also compares favorably to popular VL models that are trained on much larger datasets.
          </div>
        </div>
        </div>
      </div>
      </div>
    </div>
  </section>

  <section class="section body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 10px;">Visual Trace Generation & Close-loop Control with TraceVLA</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">
            
            <tr>
              <td>
                <div class="row">
                  <div class="col">
                    <img src="./static/images/control.png" alt="Image description" width="100%">
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px">Given a sequence of historical image observations, we first use Co-tracker to extract dense point trajectories and keep active point trajectories with significant movement.
                        We then overlay active point trajectories on the robot’s initial observation frame as visual trace prompting and feed both the image overlaid with visual traces and the original image into VLA as model input.</p>
                </div>
              </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 10px;">Real-Robot Rollouts</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
            <div class="content has-text-justified">
              <p>
                Below are videos of TraceVLA and OpenVLA on physical WidowX-250 robot manipulation tasks with different manipulation skills and objects. (Videos are sped up by 2.5x.)
              </p>
            </div>
            <div class="content has-text-justified">
              <p><strong>TraceVLA</strong> masters soft object manipulation, pick-and-place operations, and object movement, demonstrating reliable performance in both in-distribution and out-of-distribution generalization tasks.</p>
            </div>

            <div class="columns is-vcentered is-multiline interpolation-panel">
              <div class="column is-full" style="text-align: center; margin-bottom: 0; padding-bottom: 0;">  
                <h5 style="font-size: 24px; margin-bottom: 1px;">Pick Place Hotdog Sausage</h5>
              </div>
              <div class="column is-half" style="text-align: center; padding-top: 0;"> 
                  <div style="width: 80%; display: inline-block;">
                    <div style="text-align: center; margin-top: 1px;">
                      <strong>Magma</strong>
                    </div>
                      <video autoplay controls muted loop playsinline width="100%">
                          <source src="static/videos/fold_tracevla.mp4" type="video/mp4">
                      </video>
                  </div>
              </div>
              <div class="column is-half" style="text-align: center; padding-top: 0;"> 
                  <div style="width: 80%; display: inline-block;">
                    <div style="text-align: center; margin-top: 1px;">
                      OpenVLA
                    </div>
                      <video autoplay controls muted loop playsinline width="100%">
                          <source src="static/videos/fold_openvla.mp4" type="video/mp4">
                      </video>
                  </div>
              </div>
          </div>

            <div class="columns is-vcentered interpolation-panel">
              <div class="column has-text-centered">
                <h5>Fold Cloth</h5>
                <p style="font-size: 15px; color: #2a2727; margin: 2px 10%; line-height: 1.2;">
                  (In-Distribution)
                </p>
                <div style="width: 49%; display: inline-block;">
                  <video autoplay controls muted loop playsinline width="100%">
                      <source src="static/videos/fold_tracevla.mp4" type="video/mp4">
                  </video>
                  <div style="text-align: center; margin-top: 5px;">
                      TraceVLA
                  </div>
              </div>
              <div style="width: 49%; display: inline-block;">
                <video autoplay controls muted loop playsinline width="100%">
                    <source src="static/videos/fold_openvla.mp4" type="video/mp4">
                </video>
                <div style="text-align: center; margin-top: 5px;">
                    OpenVLA
                </div>
              </div>
              </div>
              
              <div class="column is-half  has-text-centered">
                <h5>Pickplace Corn Pot</h5>
                <p style="font-size: 15px; color: #2a2727; margin: 2px 10%; line-height: 1.2;">
                  (Out-of-Distribution: Unseen Task)
                </p>
                <div style="width: 49%; display: inline-block;">
                  <video autoplay controls muted loop playsinline width="100%">
                    <source src="static/videos/corn_tracevla.mp4" type="video/mp4">
                  </video>
                  <div style="text-align: center; margin-top: 5px;">
                    TraceVLA
                  </div>
                </div>
                <div style="width: 49%; display: inline-block;">
                  <video autoplay controls muted loop playsinline width="100%">
                    <source src="static/videos/openvla_corn.mp4" type="video/mp4">
                  </video>
                  <div style="text-align: center; margin-top: 5px;">
                    OpenVLA
                  </div>
                </div>
              </div>
            </div>

            <div class="columns is-vcentered interpolation-panel">
              <div class="column  has-text-centered">
                <h5>Pick Banana to the Right of Plate</h5>
                <p style="font-size: 15px; color: #2a2727; margin: 2px 10%; line-height: 1.2;">
                  (Out-of-Distribution: Unseen Task)
                </p>
                <div style="width: 49%; display: inline-block;">
                  <video autoplay controls muted loop playsinline width="100%">
                      <source src="static/videos/banana_tracevla.mp4" type="video/mp4">
                  </video>
                  <div style="text-align: center; margin-top: 5px;">
                      TraceVLA
                  </div>
              </div>
              <div style="width: 49%; display: inline-block;">
                <video autoplay controls muted loop playsinline width="100%">
                    <source src="static/videos/banana_openvla.mp4" type="video/mp4">
                </video>
                <div style="text-align: center; margin-top: 5px;">
                    OpenVLA
                </div>
              </div>
              </div>
              <div class="column  has-text-centered">
                <h5>Lift AAA Battery</h5>
                <p style="font-size: 15px; color: #2a2727; margin: 2px 10%; line-height: 1.2;">
                  (Out-of-Distribution: Unseen Object)
                </p>
                <div style="width: 49%; display: inline-block;">
                  <video autoplay controls muted loop playsinline width="100%">
                    <source src="static/videos/battery_tracevla.mp4" type="video/mp4">
                  </video>
                  <div style="text-align: center; margin-top: 5px;">
                    TraceVLA
                  </div>
                </div>
                <div style="width: 49%; display: inline-block;">
                  <video autoplay controls muted loop playsinline width="100%">
                    <source src="static/videos/battery_openvla.mp4" type="video/mp4">
                  </video>
                  <div style="text-align: center; margin-top: 5px;">
                    OpenVLA
                  </div>
                </div>
              </div>
            </div>
  

              <div class="columns is-centered ">
                <div class="column">
                    <p style="text-align: left;font-size: 18px">We design 8 real-world robot tasks with different manipulation skills and objects and include unseen tasks involving novel objects, goals, and language instructions for evaluating generalization in real robot settings.</p>
              </div>
            </div>
              <div class="row">
                <div class="col">
                  <img src="./static/images/magma_real.png" alt="Image description" width="80%">
                </div>
              </div>
              <div class="columns is-centered ">
                <div class="column">
                    <p style="text-align: left;font-size: 18px"><strong>TraceVLA</strong> consistently outperforms OpenVLA across diverse tasks including
                      soft object manipulation, pick-and-place operations, and object movement and demonstrates superior generalization</p>
              </div>
            </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>

 
  <section class="section body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 10px;">Simulation Benchmark: SimplerEnv</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">
            
            <tr>
              <td>
                <div class="row">
                  <div class="col">
                    <img src="./static/images/magma_simpler.png" alt="Image description" width="100%">
                  </div>
                </div>
                <div class="columns is-centered ">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px"><strong>TraceVLA</strong> consistently outperforms OpenVLA across various tasks and evaluation metrics in the SimplerEnv Google robot tasks. The improvements are evident in both the full-scale 7B models (<strong>TraceVLA</strong> vs OpenVLA) and their 4B versions (<strong>TraceVLA-Phi3</strong> vs OpenVLA-Phi3). </p>
                </div>
              </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 10px;">Training Memory Cost and Inference Speed</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">
            
            <tr>
              <td>
                <div class="row">
                  <div class="col">
                    <img src="./static/images/exp-cost.png" alt="Image description" width="100%">
                  </div>
                </div>
              <br>
              <div class="columns is-centered ">
                <div class="column">
                    <p style="text-align: left;font-size: 18px"><strong>TraceVLA</strong>'s memory overhead is manageable at less than 10GB when using 8 H100 GPUs, with the difference decreasing at smaller batch sizes. In terms of speed, the model introduces three main components during inference: image/text tokens (0.002s), CoTracker tracking (0.03s), and dense point tracking (0.004s) per timestep. These additional computational costs remain relatively small and well-optimized due to GPU attention optimization.</p>
              </div>
            </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zheng2024tracevla,
        title={TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies},
        author={Zheng, Ruijie and Liang, Yongyuan and Huang, Shuaiyi and Gao, Jianfeng and Daum{\'e} III, Hal and Kolobov, Andrey and Huang, Furong and Yang, Jianwei},
        journal={arXiv preprint arXiv:2412.10345},
        year={2024}
      }</code></pre>
    </div>
  </section>
     

  <br><br><br>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-8">
          <div class="content">
            <a style="color:hsla(194, 67%, 58%, 0.862)" href="#top"><i class="fa fa-arrow-up"></i><br/>Return to top</a>
            <p>
              Website adapted from <a style="color:hsla(194, 67%, 58%, 0.862)" href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a style="color:hsla(194, 67%, 58%, 0.862)" href="https://github.com/tracevla/tracevla.github.io">TraceVLA</a> under <a style="color:hsla(194, 67%, 58%, 0.862)"
              href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

<script>
    document.addEventListener('DOMContentLoaded', function() {
        const videos = document.getElementsByTagName('video');
        for (let video of videos) {
            video.playbackRate = 2.0;
        }
    });
</script>

</body>
</html>


