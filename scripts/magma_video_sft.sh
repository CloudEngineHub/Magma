#!/bin/bash

python -m torch.distributed.run --nproc_per_node 8 --nnodes 8 --node_rank $$NODE_RANK --master_port 9500  --master_addr node-0 train.py
      --deepspeed ./deepspeed/zero3.json
      --model_name_or_path /path/to/pretrain/model
      --version llama_3_instruct
      --data_path ./data_configs/video_instruction_tuning_cluster.yaml
      --img_size 256
      --training_size -1
      --img_anyres_strategy 'crop'
      --max_num_crops 4
      --max_num_frames 32
      --mm_use_spatial_pooling False
      --mm_use_vlm_template True
      --vision_backbone 'convnextxxlarge' 
      --feature_outs 'encoder' 
      --is_multimodal True 
      --mm_projector_type 'mlp2x_gelu_segtokv9' 
      --tune_vision_tokenizer 'all'
      --segtok_posembed 'sinusoidal' 
      --mm_vision_select_layer -2
      --mm_use_trace_start_end False
      --mm_use_trace_speed False
      --mm_use_image_start_end True
      --remove_static_trace_pts False
      --spatial_quant_size 256
      --group_by_modality_length True
      --bf16 True
      --output_dir '/path/to/output_dir'
      --num_train_epochs 1
      --per_device_train_batch_size 1
      --per_device_eval_batch_size 1
      --gradient_accumulation_steps 2
      --evaluation_strategy "no"
      --eval_steps 500
      --save_strategy "steps"
      --save_steps 50000
      --save_total_limit 1000
      --learning_rate 1e-5
      --weight_decay 0
      --warmup_ratio 0
      --min_lr_rate 0.1
      --lr_scheduler_type 'cosine'
      --logging_steps 10
      --tf32 False
      --model_max_length 16384
      --gradient_checkpointing True
      --dataloader_num_workers 8
      --lazy_preprocess True
      --flash_attn_2_enabled True
      --report_to wandb      
      --run_name video_magma_sft